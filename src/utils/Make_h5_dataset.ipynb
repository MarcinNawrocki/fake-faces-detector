{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Make_h5_dataset.ipynb","provenance":[],"authorship_tag":"ABX9TyNAzn+az5/t7nOrk5eWIYVO"},"kernelspec":{"name":"python379jvsc74a57bd00991d9f816f3ae08431cbfe77283594d685f9351f97c7a8199c52bebed5a8c9f","display_name":"Python 3.7.9 64-bit ('TF_gpu': conda)"},"language_info":{"name":"python","version":"3.7.9"}},"cells":[{"cell_type":"code","metadata":{"id":"3MOgtxRHq-l2","executionInfo":{"status":"ok","timestamp":1618476464024,"user_tz":-120,"elapsed":602,"user":{"displayName":"Marcin Nawrocki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWNgsnwLeRdQhCdvgSiWdtmpF5uLrNe1NL5s4uAw=s64","userId":"11241534018470902834"}}},"source":["import os\n","from PIL import Image\n","import random\n","\n","import h5py\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from db_helper import get_files_paths_recursive"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["categories_and_paths = {0: R\"F:\\master-thesis-databases\\classification_db\\fake\", \n","                        1 :R\"F:\\master-thesis-databases\\classification_db\\real\"}\n","result_dir = R\"C:\\Users\\Marcin\\Dysk Google\\masterDB\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_dataset_h5(path, dataset_name):\n","    with h5py.File(path, \"r\") as hf:\n","        print(hf.keys())\n","        X = hf[dataset_name][:]\n","        hf.close()\n","    return X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def convert_images_to_h5(img_paths, h5_path, dataset_name, img_in_cycle):\n","    print(f\"Saving images as {dataset_name}\")\n","    img_list = []\n","    i=0\n","    for img_path in img_paths:\n","        if i % 100 == 0:\n","            print(f\"Image number: {i}\")\n","        i += 1\n","        # loading images \n","        pil_image = Image.open(img_path)\n","        np_image = np.array(pil_image).astype((np.uint8))\n","        img_list.append(np_image)\n","        # saving to h5 file\n","        if i % img_in_cycle == 0 and i > 0:\n","            # h5 dataset creation\n","            np_img_list = np.asarray(img_list)\n","            #TODO maybe some static function\n","            if i == img_in_cycle:\n","                with h5py.File(h5_path, 'w') as hf:\n","                    #images\n","                    hf.create_dataset(dataset_name, \n","                                    np_img_list.shape, \n","                                    data=np_img_list,\n","                                    maxshape=((None,)+np_img_list.shape[1:]),\n","                                    chunks=True)\n","            # h5 dataset append\n","            else:\n","                with h5py.File(h5_path, \"a\") as hf:\n","                    hf[dataset_name].resize(i, axis=0)\n","                    hf[dataset_name][-img_in_cycle:] = np.asarray(img_list)\n","                    hf.close()   \n","            img_list.clear()"]},{"cell_type":"code","metadata":{"id":"i2Uk2KTerWWU","executionInfo":{"status":"ok","timestamp":1618476590958,"user_tz":-120,"elapsed":712,"user":{"displayName":"Marcin Nawrocki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWNgsnwLeRdQhCdvgSiWdtmpF5uLrNe1NL5s4uAw=s64","userId":"11241534018470902834"}}},"source":["def convert_dataset_to_h5(img_paths, img_categories, h5_path, dataset_name, img_in_cycle=10):\n","    print(f\"To {h5_path} for dataset:{dataset_name}\")\n","    assert len(img_paths) > img_in_cycle, \"Database to small for specified saving cycle\"\n","\n","    if os.path.exists(h5_path):\n","        os.remove(h5_path)\n","    X_dataset_name = 'X_' + dataset_name\n","    y_dataset_name = 'y_' + dataset_name \n","\n","    # add images\n","    convert_images_to_h5(img_paths, h5_path, X_dataset_name, img_in_cycle=img_in_cycle)\n","    # add categories\n","    np_img_categories = np.asarray(img_categories)\n","    with h5py.File(h5_path, 'a') as hf:\n","        #images\n","        hf.create_dataset(y_dataset_name, \n","                        np_img_categories.shape, \n","                        data=np_img_categories,\n","                        maxshape=np_img_categories.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def make_dataset_for_keras_h5(categories, result_directory, split_factor=0.8):\n","    print(f\"Make dataset from: {categories} to {result_directory}\")\n","    flag='w'\n","    train_path = os.path.join(result_directory, 'train.h5')\n","    val_path = os.path.join(result_directory, 'val.h5')\n","    img_paths_and_categories = []\n","    for name, path in categories.items():\n","        img_paths_and_categories += get_files_paths_recursive(path, category=name)[:5000]\n","    # create shuffled lists\n","    random.shuffle(img_paths_and_categories)\n","    img_paths = [path for path, category in img_paths_and_categories]\n","    img_categories = [category for path, category in img_paths_and_categories]\n","    # create datasets\n","    train_val_border = int(split_factor*len(img_paths_and_categories))\n","    convert_dataset_to_h5(img_paths[:train_val_border], img_categories[:train_val_border], train_path, \"train\")\n","    convert_dataset_to_h5(img_paths[train_val_border:], img_categories[train_val_border:], val_path, \"val\")\n","       \n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["make_dataset_for_keras_h5(categories_and_paths, result_dir)"]}]}