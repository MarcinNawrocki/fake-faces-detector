{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":602,"status":"ok","timestamp":1618476464024,"user":{"displayName":"Marcin Nawrocki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWNgsnwLeRdQhCdvgSiWdtmpF5uLrNe1NL5s4uAw=s64","userId":"11241534018470902834"},"user_tz":-120},"id":"3MOgtxRHq-l2"},"outputs":[],"source":["import os\n","from PIL import Image\n","import random\n","import typing as t\n","\n","import h5py\n","import numpy as np\n","\n","from db_helper import get_files_paths_recursive"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["categories_and_paths = {0: R\"F:\\master-thesis-databases\\classification_db\\fake\", \n","                        1 :R\"F:\\master-thesis-databases\\classification_db\\real\"}\n","result_dir = R\"C:\\Users\\Marcin\\Dysk Google\\masterDB\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def load_dataset_h5(path, dataset_name):\n","    with h5py.File(path, \"r\") as hf:\n","        print(hf.keys())\n","        X = hf[dataset_name][:]\n","        hf.close()\n","    return X"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def convert_images_to_h5(img_paths: t.List, h5_path: str, dataset_name: str, img_in_cycle: int):\n","    print(f\"Saving images as {dataset_name}\")\n","    img_list = []\n","    i=0\n","    for img_path in img_paths:\n","        if i % 100 == 0:\n","            print(f\"Image number: {i}\")\n","        i += 1\n","        # loading images \n","        pil_image = Image.open(img_path)\n","        np_image = np.array(pil_image).astype((np.uint8))\n","        img_list.append(np_image)\n","        # saving to h5 file\n","        if i % img_in_cycle == 0 and i > 0:\n","            # h5 dataset creation\n","            np_img_list = np.asarray(img_list)\n","\n","            if i == img_in_cycle:\n","                with h5py.File(h5_path, 'w') as hf:\n","                    #images\n","                    hf.create_dataset(dataset_name, \n","                                    np_img_list.shape, \n","                                    data=np_img_list,\n","                                    maxshape=((None,)+np_img_list.shape[1:]),\n","                                    chunks=True)\n","            # h5 dataset append\n","            else:\n","                with h5py.File(h5_path, \"a\") as hf:\n","                    hf[dataset_name].resize(i, axis=0)\n","                    hf[dataset_name][-img_in_cycle:] = np.asarray(img_list)\n","                    hf.close()   \n","            img_list.clear()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":712,"status":"ok","timestamp":1618476590958,"user":{"displayName":"Marcin Nawrocki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgWNgsnwLeRdQhCdvgSiWdtmpF5uLrNe1NL5s4uAw=s64","userId":"11241534018470902834"},"user_tz":-120},"id":"i2Uk2KTerWWU"},"outputs":[],"source":["def convert_dataset_to_h5(img_paths, img_categories, h5_path, dataset_name, img_in_cycle=10):\n","    print(f\"To {h5_path} for dataset:{dataset_name}\")\n","    assert len(img_paths) > img_in_cycle, \"Database to small for specified saving cycle\"\n","\n","    if os.path.exists(h5_path):\n","        os.remove(h5_path)\n","    X_dataset_name = 'X_' + dataset_name\n","    y_dataset_name = 'y_' + dataset_name \n","\n","    # add images\n","    convert_images_to_h5(img_paths, h5_path, X_dataset_name, img_in_cycle=img_in_cycle)\n","    # add categories\n","    np_img_categories = np.asarray(img_categories)\n","    with h5py.File(h5_path, 'a') as hf:\n","        #images\n","        hf.create_dataset(y_dataset_name, \n","                        np_img_categories.shape, \n","                        data=np_img_categories,\n","                        maxshape=np_img_categories.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def make_dataset_for_keras_h5(categories, result_directory, split_factor=0.8):\n","    print(f\"Make dataset from: {categories} to {result_directory}\")\n","    flag='w'\n","    train_path = os.path.join(result_directory, 'train.h5')\n","    val_path = os.path.join(result_directory, 'val.h5')\n","    img_paths_and_categories = []\n","    for name, path in categories.items():\n","        img_paths_and_categories += get_files_paths_recursive(path, category=name)[:5000]\n","    # create shuffled lists\n","    random.shuffle(img_paths_and_categories)\n","    img_paths = [path for path, category in img_paths_and_categories]\n","    img_categories = [category for path, category in img_paths_and_categories]\n","    # create datasets\n","    train_val_border = int(split_factor*len(img_paths_and_categories))\n","    print(train_val_border, len(img_paths_and_categories))\n","    convert_dataset_to_h5(img_paths[:train_val_border], img_categories[:train_val_border], train_path, \"train\")\n","    convert_dataset_to_h5(img_paths[train_val_border:], img_categories[train_val_border:], val_path, \"val\")\n","       \n"]},{"cell_type":"code","execution_count":7,"metadata":{"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Make dataset from: {0: 'F:\\\\master-thesis-databases\\\\classification_db\\\\fake', 1: 'F:\\\\master-thesis-databases\\\\classification_db\\\\real'} to C:\\Users\\Marcin\\Dysk Google\\masterDB\n","8000 10000\n","To C:\\Users\\Marcin\\Dysk Google\\masterDB\\train.h5 for dataset:train\n","Saving images as X_train\n","Image number: 0\n","Image number: 100\n","Image number: 200\n","Image number: 300\n","Image number: 400\n","Image number: 500\n","Image number: 600\n","Image number: 700\n","Image number: 800\n","Image number: 900\n","Image number: 1000\n","Image number: 1100\n","Image number: 1200\n","Image number: 1300\n","Image number: 1400\n","Image number: 1500\n","Image number: 1600\n","Image number: 1700\n","Image number: 1800\n","Image number: 1900\n","Image number: 2000\n","Image number: 2100\n","Image number: 2200\n","Image number: 2300\n","Image number: 2400\n","Image number: 2500\n","Image number: 2600\n","Image number: 2700\n","Image number: 2800\n","Image number: 2900\n","Image number: 3000\n","Image number: 3100\n","Image number: 3200\n","Image number: 3300\n","Image number: 3400\n","Image number: 3500\n","Image number: 3600\n","Image number: 3700\n","Image number: 3800\n","Image number: 3900\n","Image number: 4000\n","Image number: 4100\n","Image number: 4200\n","Image number: 4300\n","Image number: 4400\n","Image number: 4500\n","Image number: 4600\n","Image number: 4700\n","Image number: 4800\n","Image number: 4900\n","Image number: 5000\n","Image number: 5100\n","Image number: 5200\n","Image number: 5300\n","Image number: 5400\n","Image number: 5500\n","Image number: 5600\n","Image number: 5700\n","Image number: 5800\n","Image number: 5900\n","Image number: 6000\n","Image number: 6100\n","Image number: 6200\n","Image number: 6300\n","Image number: 6400\n","Image number: 6500\n","Image number: 6600\n","Image number: 6700\n","Image number: 6800\n","Image number: 6900\n","Image number: 7000\n","Image number: 7100\n","Image number: 7200\n","Image number: 7300\n","Image number: 7400\n","Image number: 7500\n","Image number: 7600\n","Image number: 7700\n","Image number: 7800\n","Image number: 7900\n","To C:\\Users\\Marcin\\Dysk Google\\masterDB\\val.h5 for dataset:val\n","Saving images as X_val\n","Image number: 0\n","Image number: 100\n","Image number: 200\n","Image number: 300\n","Image number: 400\n","Image number: 500\n","Image number: 600\n","Image number: 700\n","Image number: 800\n","Image number: 900\n","Image number: 1000\n","Image number: 1100\n","Image number: 1200\n","Image number: 1300\n","Image number: 1400\n","Image number: 1500\n","Image number: 1600\n","Image number: 1700\n","Image number: 1800\n","Image number: 1900\n"]}],"source":["make_dataset_for_keras_h5(categories_and_paths, result_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNAzn+az5/t7nOrk5eWIYVO","name":"Make_h5_dataset.ipynb","provenance":[]},"interpreter":{"hash":"0991d9f816f3ae08431cbfe77283594d685f9351f97c7a8199c52bebed5a8c9f"},"kernelspec":{"display_name":"Python 3.7.9 64-bit ('TF_gpu': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":2}
